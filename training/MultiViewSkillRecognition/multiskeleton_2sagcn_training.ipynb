{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/thibault/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "source": [
    "from egoexo_dataloader_2 import EgoExoDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "from mmaction.apis import pose_inference, detection_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/media/thibault/T5 EVO/Datasets/Ego4D/'\n",
    "train_dataset = EgoExoDataset(dataset_path, os.path.join(dataset_path, 'takes.json'), split = \"train\", skill=True, get_frames=False, get_pose=True, get_hands_pose=False, frame_rate=3, transform=None)\n",
    "val_dataset = EgoExoDataset(dataset_path, os.path.join(dataset_path, 'takes.json'), split = \"val\", skill=True, get_frames=False, get_pose=True, get_hands_pose=False, frame_rate=3, transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: /home/thibault/Documents/Code/pckg/mmaction2/checkpoints/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint_20220815-38db104b.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: cls_head.fc_cls.weight, cls_head.fc_cls.bias\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/thibault/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmengine/runner/checkpoint.py:347: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "from mmaction.apis import inference_skeleton, init_recognizer\n",
    "from mmengine import Config\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "config_path = \"/home/thibault/Documents/Code/pckg/mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py\"\n",
    "checkpoint_path = \"/home/thibault/Documents/Code/pckg/mmaction2/checkpoints/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint_20220815-38db104b.pth\" # can be a local path   # you can specify your own picture path\n",
    "\n",
    "cfg = Config.fromfile(config_path)\n",
    "\n",
    "\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "skeleton_model = init_recognizer(cfg, checkpoint_path, device=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_pose_and_label_with_padding(batch):\n",
    "    \"\"\"\n",
    "    batch: list of samples, each a dict with:kp\n",
    "      - 'pose':  np.ndarray (dtype=object) length N_i of view‐dicts\n",
    "      - 'label': int, V_max)\n",
    "      labels:           LongTensor  (B,)\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    # 1) labels\n",
    "    labels = torch.tensor([s['skill'] for s in batch], dtype=torch.long)\n",
    "\n",
    "    # 2) collect pose-arrays and find max views\n",
    "    pose_arrays = [s['pose'] for s in batch]            # list of np.object arrays\n",
    "    num_views   = [len(pa) for pa in pose_arrays]       # N_i\n",
    "    V_max       = max(num_views)\n",
    "\n",
    "    for sample in batch:\n",
    "      if len(sample[\"pose\"].keys()) < V_max:\n",
    "        for i in range(len(sample[\"pose\"].keys()), V_max):\n",
    "          T = len(sample[\"pose\"][\"0\"])\n",
    "          sample[\"pose\"][str(i)] = np.array(np.repeat({\"keypoints\": np.zeros((V_max, 17, 2)), \"keypoint_scores\": np.zeros((V_max, 17))}, T))\n",
    "       \n",
    "        \n",
    "      \n",
    "    \n",
    "    return pose_arrays, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_objects(batch):\n",
    "    \"\"\"\n",
    "    batch: list of samples, each a dict with:kp\n",
    "      - 'pose':  np.ndarray (dtype=object) length N_i of view‐dicts\n",
    "      - 'label': int, V_max)\n",
    "      labels:           LongTensor  (B,)\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    # 1) labels\n",
    "    labels = torch.tensor([s['skill'] for s in batch], dtype=torch.long)\n",
    "\n",
    "    # 2) collect pose-arrays and find max views\n",
    "    pose_arrays = [s for s in batch]            # list of np.object arrays\n",
    "        \n",
    "      \n",
    "    \n",
    "    return pose_arrays, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DETECTION_CONFIG = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"faster-rcnn_r50_fpn_2x_coco_infer.py\"\n",
    ")\n",
    "DETECTION_CHECKPOINT = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\"\n",
    ")\n",
    "\n",
    "SKELETON_CONFIG = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py\"\n",
    ")\n",
    "SKELETON_CHECKPOINT = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"hrnet_w32_coco_256x192-c78dce93_20200708.pth\"\n",
    ")\n",
    "\n",
    "WINDOW_SIZE = 128\n",
    "SAMPLING_RATE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Callable, Generator, List\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import contextlib\n",
    "\n",
    "\n",
    "def frame_windows(video_path: str,\n",
    "                  window_size: int,\n",
    "                  sampling_rate: int = 1\n",
    "                 ) -> Generator[List[np.ndarray], None, None]:\n",
    "    \"\"\"\n",
    "    Lazily read frames from the video, sampling 1 in every `sampling_rate`,\n",
    "    and yield lists of up to `window_size` sampled frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(f'Length of video: {cap.get(cv2.CAP_PROP_FRAME_COUNT)}')\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {video_path}\")\n",
    "        return None\n",
    "    frames: List[np.ndarray] = []\n",
    "    raw_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # only keep every `sampling_rate`th frame\n",
    "        if raw_idx % sampling_rate == 0:\n",
    "            frames.append(frame)\n",
    "            if len(frames) == window_size:\n",
    "                yield frames\n",
    "                frames = []\n",
    "\n",
    "        raw_idx += 1\n",
    "\n",
    "    # leftover sampled frames\n",
    "    if frames:\n",
    "        yield frames\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "def extract_skeleton(video_path: str,\n",
    "                     window_size: int = 32,\n",
    "                     sampling_rate: int = 1,\n",
    "                     predict: Callable[[List[np.ndarray]], np.ndarray] = None\n",
    "                    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Process a video in non-overlapping windows to extract per-frame skeletons\n",
    "    without ever loading the whole video into RAM at once.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to the input video file.\n",
    "        window_size: Number of frames per chunk.\n",
    "        predict: Function(frames_list) → np.ndarray of shape\n",
    "                 (num_frames, num_keypoints, dims).\n",
    "\n",
    "    Returns:\n",
    "        skeleton_sequence: np.ndarray of shape\n",
    "            (total_frames, num_keypoints, dims).\n",
    "    \"\"\"\n",
    "    if predict is None:\n",
    "        raise ValueError(\"You must pass in a `predict` function.\")\n",
    "    \n",
    "    skeleton_chunks = []\n",
    "    total = 0\n",
    "    for frames in frame_windows(video_path, window_size, sampling_rate):\n",
    "        # frames is at most window_size in length\n",
    "\n",
    "# Suppose this is the noisy function:\n",
    "# from some_lib import noisy_function\n",
    "\n",
    "        with open(os.devnull, 'w') as devnull, \\\n",
    "            contextlib.redirect_stdout(devnull), \\\n",
    "            contextlib.redirect_stderr(devnull):\n",
    "            # Anything printed to stdout or stderr inside here is discarded\n",
    "            \n",
    "\n",
    "            det_results, _ = detection_inference(str(DETECTION_CONFIG), str(DETECTION_CHECKPOINT), frames)\n",
    "            pose_results, pose_data_samples = pose_inference(str(SKELETON_CONFIG), str(SKELETON_CHECKPOINT), frames, det_results)\n",
    "            # e.g. (len(frames), K, D)\n",
    "            skeleton_chunks.append(np.array(pose_results))\n",
    "            total += len(pose_results)\n",
    "        print(total)\n",
    "    \n",
    "    # Concatenate along temporal axis:\n",
    "    if len(skeleton_chunks) == 0:\n",
    "        skeleton_sequence = np.array([])\n",
    "    else:\n",
    "        skeleton_sequence = np.concatenate(skeleton_chunks, axis=0)\n",
    "    assert skeleton_sequence.shape[0] == total\n",
    "\n",
    "    return skeleton_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_skeletons(poses, skeleton_model, pose_inference, window_size=128, sampling_rate=3):\n",
    "    \"\"\"\n",
    "    Process a batch of skeletons.\n",
    "    Args:\n",
    "        poses: list of dicts with keys \"ego\" and \"exo\"\n",
    "        pose_inference: function to extract skeletons\n",
    "    Returns:\n",
    "        batch: list of tensors with shape (B, T, D)\n",
    "    \"\"\"\n",
    "    skeletons_batch = []\n",
    "    for data in poses:\n",
    "        paths = [path for path in data[\"samples\"][\"exo\"]]\n",
    "        paths.append(data[\"samples\"][\"ego\"])\n",
    "        \n",
    "        skeletons = {}\n",
    "        \n",
    "        for i, path in enumerate(paths):\n",
    "            # Extract skeletons from the video\n",
    "            skeletons[str(i)] = extract_skeleton(path, window_size, sampling_rate, predict=pose_inference)\n",
    "        skeletons_batch.append(skeletons)\n",
    "    batch = []\n",
    "    \n",
    "    for sample in skeletons_batch:\n",
    "        views = []\n",
    "        for view in sample.values():\n",
    "            representation = inference_skeleton(skeleton_model, view,(1920,1080), test_pipeline=None)  # (B, T, D)\n",
    "            views.append(representation)  # (B, T, D)\n",
    "        batch.append(torch.stack(views))  # (B, T, D)\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "logging.getLogger('mmdet').setLevel(logging.WARNING)\n",
    "# Now INFO and DEBUG from some_lib won’t show up.\n",
    "\n",
    "\n",
    "\n",
    "class MultiViewSkeletonClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch model for multi-view skeleton fusion and classification.\n",
    "\n",
    "    Inputs:\n",
    "        - Four skeleton feature tensors, each of shape (batch_size, seq_len=20, feat_dim=512).\n",
    "    Output:\n",
    "        - Logits for classification into num_classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, skeleton_model, feat_dim=512, seq_len=20, num_views=4, hidden_dim=512, num_classes=10):\n",
    "        super(MultiViewSkeletonClassifier, self).__init__()\n",
    "        self.num_views = num_views\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.skeleton_model = skeleton_model\n",
    "        \n",
    "        # Define the attention head for per-view summary\n",
    "        self.attn_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # Learnable fusion weights for each view (softmax-normalized)\n",
    "        self.view_weights = nn.Parameter(torch.ones(num_views))\n",
    "\n",
    "        # Project concatenated features to hidden dimension\n",
    "        self.fusion_proj = nn.Linear(feat_dim, hidden_dim)\n",
    "\n",
    "        # Temporal convolutional layers\n",
    "        self.conv1 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * seq_len, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            views: list or tuple of length num_views, each tensor of shape (B, T, D)\n",
    "        Returns:\n",
    "            logits: tensor of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Stack views: (V, B, T, D) -> (B, V, T, D)\n",
    "        \n",
    "            \n",
    "        x = torch.stack(batch).to(device='cuda:0')  # (B, V, T, D)\n",
    "        B, V, T, D = x.shape\n",
    "        \n",
    "        # per‐view summary → score → masked softmax → fusion\n",
    "        summary = x.mean(dim=2)              # (B, V, hidden)\n",
    "        raw_w   = self.attn_head(summary).squeeze(-1) # (B, V)\n",
    "        w       = F.softmax(raw_w)    # (B, V)\n",
    "        fused   = (w.view(B,V,1,1) * x).sum(dim=1)\n",
    "\n",
    "\n",
    "        # Project features\n",
    "        fused = self.fusion_proj(fused)  # (B, T, hidden_dim)\n",
    "        fused = F.relu(fused)\n",
    "\n",
    "        # Prepare for temporal conv: (B, hidden_dim, T)\n",
    "        fused = fused.permute(0, 2, 1)\n",
    "\n",
    "        # Temporal convolutional block\n",
    "        out = F.relu(self.bn1(self.conv1(fused)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "\n",
    "        # Flatten temporal features\n",
    "        out = out.view(B, -1)\n",
    "\n",
    "        # Classification head\n",
    "        logits = self.fc(out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/30:   0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of video: 1453.0\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 9.2 task/s, elapsed: 3s, ETA:     0s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 14.0 task/s, elapsed: 2s, ETA:     0s\n",
      "23\n",
      "Length of video: 1453.0\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 9.3 task/s, elapsed: 2s, ETA:     0s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 13.7 task/s, elapsed: 2s, ETA:     0s\n",
      "23\n",
      "Length of video: 1453.0\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 9.2 task/s, elapsed: 2s, ETA:     0s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 14.4 task/s, elapsed: 2s, ETA:     0s\n",
      "23\n",
      "Length of video: 1453.0\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 9.3 task/s, elapsed: 2s, ETA:     0s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 14.0 task/s, elapsed: 2s, ETA:     0s\n",
      "23\n",
      "Length of video: 1453.0\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 9.2 task/s, elapsed: 3s, ETA:     0s\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 23/23, 14.3 task/s, elapsed: 2s, ETA:     0s\n",
      "23\n",
      "Length of video: 1453.0\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>       ] 18/23, 12.9 task/s, elapsed: 1s, ETA:     0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch 1/30:   0%|          | 0/71 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m labels = labels.to(device)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# forward + backward\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m batch = \u001b[43mprocess_skeletons\u001b[49m\u001b[43m(\u001b[49m\u001b[43mviews\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskeleton_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_inference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m outputs = model(batch)\n\u001b[32m     62\u001b[39m loss    = criterion(outputs, labels)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mprocess_skeletons\u001b[39m\u001b[34m(poses, skeleton_model, pose_inference)\u001b[39m\n\u001b[32m     15\u001b[39m     skeletons = {}\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(paths):\n\u001b[32m     18\u001b[39m         \u001b[38;5;66;03m# Extract skeletons from the video\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m         skeletons[\u001b[38;5;28mstr\u001b[39m(i)] = \u001b[43mextract_skeleton\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpose_inference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     skeletons_batch.append(skeletons)\n\u001b[32m     21\u001b[39m batch = []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mextract_skeleton\u001b[39m\u001b[34m(video_path, window_size, sampling_rate, predict)\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m frames \u001b[38;5;129;01min\u001b[39;00m frame_windows(video_path, window_size, sampling_rate):\n\u001b[32m     69\u001b[39m         \u001b[38;5;66;03m# frames is at most window_size in length\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Suppose this is the noisy function:\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# from some_lib import noisy_function\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.devnull, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m devnull, \\\n\u001b[32m     75\u001b[39m             contextlib.redirect_stdout(devnull), \\\n\u001b[32m     76\u001b[39m             contextlib.redirect_stderr(devnull):\n\u001b[32m     77\u001b[39m             \u001b[38;5;66;03m# Anything printed to stdout or stderr inside here is discarded\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m             det_results, _ = \u001b[43mdetection_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDETECTION_CONFIG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDETECTION_CHECKPOINT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m             pose_results, pose_data_samples = pose_inference(\u001b[38;5;28mstr\u001b[39m(SKELETON_CONFIG), \u001b[38;5;28mstr\u001b[39m(SKELETON_CHECKPOINT), frames, det_results)\n\u001b[32m     82\u001b[39m             \u001b[38;5;66;03m# e.g. (len(frames), K, D)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Code/pckg/mmaction2/mmaction/apis/inference.py:218\u001b[39m, in \u001b[36mdetection_inference\u001b[39m\u001b[34m(det_config, det_checkpoint, frame_paths, det_score_thr, det_cat_id, device, with_score)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mPerforming Human Detection for each frame\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m frame_path \u001b[38;5;129;01min\u001b[39;00m track_iter_progress(frame_paths):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     det_data_sample: DetDataSample = \u001b[43minference_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m     pred_instance = det_data_sample.pred_instances.cpu().numpy()\n\u001b[32m    220\u001b[39m     bboxes = pred_instance.bboxes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmdet/apis/inference.py:189\u001b[39m, in \u001b[36minference_detector\u001b[39m\u001b[34m(model, imgs, test_pipeline, text_prompt, custom_entities)\u001b[39m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# forward the model\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m         results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    191\u001b[39m     result_list.append(results)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmengine/model/base_model/base_model.py:145\u001b[39m, in \u001b[36mBaseModel.test_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"``BaseModel`` implements ``test_step`` the same as ``val_step``.\u001b[39;00m\n\u001b[32m    137\u001b[39m \n\u001b[32m    138\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m \u001b[33;03m    list: The predictions of given data.\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    144\u001b[39m data = \u001b[38;5;28mself\u001b[39m.data_preprocessor(data, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpredict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmengine/model/base_model/base_model.py:361\u001b[39m, in \u001b[36mBaseModel._run_forward\u001b[39m\u001b[34m(self, data, mode)\u001b[39m\n\u001b[32m    351\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[32m    352\u001b[39m \n\u001b[32m    353\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m \u001b[33;03m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m    363\u001b[39m     results = \u001b[38;5;28mself\u001b[39m(*data, mode=mode)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmdet/models/detectors/base.py:94\u001b[39m, in \u001b[36mBaseDetector.forward\u001b[39m\u001b[34m(self, inputs, data_samples, mode)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(inputs, data_samples)\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mpredict\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mtensor\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward(inputs, data_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmdet/models/detectors/two_stage.py:231\u001b[39m, in \u001b[36mTwoStageDetector.predict\u001b[39m\u001b[34m(self, batch_inputs, batch_data_samples, rescale)\u001b[39m\n\u001b[32m    229\u001b[39m \u001b[38;5;66;03m# If there are no pre-defined proposals, use RPN to get proposals\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_data_samples[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33mproposals\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     rpn_results_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrpn_head\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    234\u001b[39m     rpn_results_list = [\n\u001b[32m    235\u001b[39m         data_sample.proposals \u001b[38;5;28;01mfor\u001b[39;00m data_sample \u001b[38;5;129;01min\u001b[39;00m batch_data_samples\n\u001b[32m    236\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmdet/models/dense_heads/base_dense_head.py:197\u001b[39m, in \u001b[36mBaseDenseHead.predict\u001b[39m\u001b[34m(self, x, batch_data_samples, rescale)\u001b[39m\n\u001b[32m    191\u001b[39m batch_img_metas = [\n\u001b[32m    192\u001b[39m     data_samples.metainfo \u001b[38;5;28;01mfor\u001b[39;00m data_samples \u001b[38;5;129;01min\u001b[39;00m batch_data_samples\n\u001b[32m    193\u001b[39m ]\n\u001b[32m    195\u001b[39m outs = \u001b[38;5;28mself\u001b[39m(x)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_by_feat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43mouts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrescale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmdet/models/dense_heads/base_dense_head.py:279\u001b[39m, in \u001b[36mBaseDenseHead.predict_by_feat\u001b[39m\u001b[34m(self, cls_scores, bbox_preds, score_factors, batch_img_metas, cfg, rescale, with_nms)\u001b[39m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    277\u001b[39m         score_factor_list = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_levels)]\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_by_feat_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_score_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_score_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbbox_pred_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbbox_pred_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_factor_list\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscore_factor_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmlvl_priors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmlvl_priors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg_meta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrescale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwith_nms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_nms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     result_list.append(results)\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result_list\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/ego-challenge/lib/python3.12/site-packages/mmdet/models/dense_heads/rpn_head.py:198\u001b[39m, in \u001b[36mRPNHead._predict_by_feat_single\u001b[39m\u001b[34m(self, cls_score_list, bbox_pred_list, score_factor_list, mlvl_priors, img_meta, cfg, rescale, with_nms)\u001b[39m\n\u001b[32m    195\u001b[39m cls_score = cls_score.permute(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m,\n\u001b[32m    196\u001b[39m                               \u001b[32m0\u001b[39m).reshape(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.cls_out_channels)\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_sigmoid_cls:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     scores = \u001b[43mcls_score\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    200\u001b[39m     \u001b[38;5;66;03m# remind that we set FG labels to [0] since mmdet v2.0\u001b[39;00m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# BG cat_id: 1\u001b[39;00m\n\u001b[32m    202\u001b[39m     scores = cls_score.softmax(-\u001b[32m1\u001b[39m)[:, :-\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs    = 30\n",
    "learning_rate = 0.001\n",
    "batch_size    = 8\n",
    "num_classes   = 5\n",
    "\n",
    "# Model\n",
    "multiview_model = MultiViewSkeletonClassifier(\n",
    "    skeleton_model=skeleton_model,\n",
    "    feat_dim=512,\n",
    "    seq_len=20,\n",
    "    num_views=4,\n",
    "    hidden_dim=512,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "model = multiview_model\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_objects,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,  # assuming you have a separate validation dataset\n",
    "    collate_fn=collate_objects,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Training + Validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for views, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # if views is a list of tensors; otherwise .to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward + backward\n",
    "        batch = process_skeletons(views, skeleton_model, pose_inference, window_size=128, sampling_rate=3)\n",
    "        outputs = model(batch)\n",
    "        loss    = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # stats\n",
    "        running_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        total   += labels.size(0)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        \n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc  = 100. * correct / total\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for views, labels in tqdm(val_loader, desc=f\"Val   Epoch {epoch+1}/{num_epochs}\"):\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "            batch = process_skeletons(views, skeleton_model, pose_inference, window_size=128, sampling_rate=3)\n",
    "            outputs = model(batch)\n",
    "            loss    = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            val_total   += labels.size(0)\n",
    "            val_correct += preds.eq(labels).sum().item()\n",
    "\n",
    "    val_loss_epoch = val_loss / len(val_loader)\n",
    "    val_acc        = 100. * val_correct / val_total\n",
    "    scheduler.step(val_loss_epoch)\n",
    "    # ---- EPOCH SUMMARY ----\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "        f\"Val   Loss: {val_loss_epoch:.4f}, Val   Acc: {val_acc:.2f}%\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), f\"multiview_model_epoch_{epoch+1}.pth\")\n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ego-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
