{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from egoexo_dataloader_2 import EgoExoDataset\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from mmpose.apis import MMPoseInferencer\n",
    "from mmaction.apis import pose_inference, detection_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/media/thibault/T5 EVO/Datasets/Ego4D/'\n",
    "train_dataset = EgoExoDataset(dataset_path, os.path.join(dataset_path, 'takes.json'), split = \"train\", skill=True, get_frames=False, get_pose=True, get_hands_pose=False, frame_rate=3, transform=None)\n",
    "val_dataset = EgoExoDataset(dataset_path, os.path.join(dataset_path, 'takes.json'), split = \"val\", skill=True, get_frames=False, get_pose=True, get_hands_pose=False, frame_rate=3, transform=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset.__getitem__(50)\n",
    "data[\"pose\"][\"0\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [path for path in data[\"samples\"][\"exo\"]]\n",
    "paths.append(data[\"samples\"][\"ego\"])\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeletons = list(data[\"pose\"].values())\n",
    "\n",
    "\n",
    "for skeleton in skeletons:\n",
    "    for frame in skeleton:\n",
    "        print(np.min(frame[\"keypoints\"]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmaction.apis import inference_skeleton, init_recognizer\n",
    "from mmengine import Config\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "config_path = \"/home/thibault/Documents/Code/pckg/mmaction2/configs/skeleton/posec3d/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint.py\"\n",
    "checkpoint_path = \"/home/thibault/Documents/Code/pckg/mmaction2/checkpoints/slowonly_r50_8xb16-u48-240e_ntu60-xsub-keypoint_20220815-38db104b.pth\" # can be a local path   # you can specify your own picture path\n",
    "\n",
    "cfg = Config.fromfile(config_path)\n",
    "\n",
    "\n",
    "\n",
    "# build the model from a config file and a checkpoint file\n",
    "skeleton_model = init_recognizer(cfg, checkpoint_path, device=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_pose_and_label_with_padding(batch):\n",
    "    \"\"\"\n",
    "    batch: list of samples, each a dict with:\n",
    "      - 'pose':  np.ndarray (dtype=object) length N_i of view‐dicts\n",
    "      - 'label': int\n",
    "\n",
    "    Returns:\n",
    "      padded_keypoints: FloatTensor (B, V_max, K, 2)\n",
    "      padded_scores:    FloatTensor (B, V_max, K)\n",
    "      view_mask:        BoolTensor  (B, V_max)\n",
    "      labels:           LongTensor  (B,)\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    # 1) labels\n",
    "    labels = torch.tensor([s['label'] for s in batch], dtype=torch.long)\n",
    "\n",
    "    # 2) collect pose-arrays and find max views\n",
    "    pose_arrays = [s['pose'] for s in batch]            # list of np.object arrays\n",
    "    num_views   = [len(pa) for pa in pose_arrays]       # N_i\n",
    "    V_max       = max(num_views)\n",
    "\n",
    "    # 3) get joint-count K (and dim=2) from the first view of first sample\n",
    "    sample0_view0 = pose_arrays[0]['0'][0]\n",
    "    _, K, dim = sample0_view0['keypoints'].shape        # (1, K, 2)\n",
    "\n",
    "    # 4) allocate padded tensors\n",
    "    padded_keypoints = torch.zeros(B, V_max, K, dim, dtype=torch.float)\n",
    "    padded_scores    = torch.zeros(B, V_max, K,    dtype=torch.float)\n",
    "    view_mask        = torch.zeros(B, V_max,        dtype=torch.bool)\n",
    "\n",
    "    # 5) fill in real views\n",
    "    for b, pa in enumerate(pose_arrays):\n",
    "        for v, view_dict in enumerate(pa.values()):\n",
    "            # squeeze out that leading 1 if present\n",
    "            kp = view_dict['keypoints']\n",
    "            if kp.ndim == 3 and kp.shape[0] == 1:\n",
    "                kp = kp[0]            # now (K, 2)\n",
    "            scores = view_dict['keypoint_scores']  # (K,)\n",
    "\n",
    "            padded_keypoints[b, v] = torch.from_numpy(kp)\n",
    "            padded_scores[b, v]    = torch.from_numpy(scores)\n",
    "            view_mask[b, v]        = True\n",
    "\n",
    "    return padded_keypoints, padded_scores, view_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_pose_and_label_with_padding_2(batch):\n",
    "    \"\"\"\n",
    "    batch: list of samples, each a dict with:kp\n",
    "      - 'pose':  np.ndarray (dtype=object) length N_i of view‐dicts\n",
    "      - 'label': int, V_max)\n",
    "      labels:           LongTensor  (B,)\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    # 1) labels\n",
    "    labels = torch.tensor([s['skill'] for s in batch], dtype=torch.long)\n",
    "\n",
    "    # 2) collect pose-arrays and find max views\n",
    "    pose_arrays = [s['pose'] for s in batch]            # list of np.object arrays\n",
    "    num_views   = [len(pa) for pa in pose_arrays]       # N_i\n",
    "    V_max       = max(num_views)\n",
    "\n",
    "    for sample in batch:\n",
    "      if len(sample[\"pose\"].keys()) < V_max:\n",
    "        for i in range(len(sample[\"pose\"].keys()), V_max):\n",
    "          T = len(sample[\"pose\"][\"0\"])\n",
    "          sample[\"pose\"][str(i)] = np.array(np.repeat({\"keypoints\": np.zeros((V_max, 17, 2)), \"keypoint_scores\": np.zeros((V_max, 17))}, T))\n",
    "       \n",
    "        \n",
    "      \n",
    "    \n",
    "    return pose_arrays, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_objects(batch):\n",
    "    \"\"\"\n",
    "    batch: list of samples, each a dict with:kp\n",
    "      - 'pose':  np.ndarray (dtype=object) length N_i of view‐dicts\n",
    "      - 'label': int, V_max)\n",
    "      labels:           LongTensor  (B,)\n",
    "    \"\"\"\n",
    "    B = len(batch)\n",
    "    # 1) labels\n",
    "    labels = torch.tensor([s['skill'] for s in batch], dtype=torch.long)\n",
    "\n",
    "    # 2) collect pose-arrays and find max views\n",
    "    pose_arrays = [s for s in batch]            # list of np.object arrays\n",
    "        \n",
    "      \n",
    "    \n",
    "    return pose_arrays, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DETECTION_CONFIG = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"faster-rcnn_r50_fpn_2x_coco_infer.py\"\n",
    ")\n",
    "DETECTION_CHECKPOINT = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\"\n",
    ")\n",
    "\n",
    "SKELETON_CONFIG = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"td-hm_hrnet-w32_8xb64-210e_coco-256x192_infer.py\"\n",
    ")\n",
    "SKELETON_CHECKPOINT = Path(\n",
    "    \"./generate_skeletons/\"\n",
    "    \"hrnet_w32_coco_256x192-c78dce93_20200708.pth\"\n",
    ")\n",
    "\n",
    "WINDOW_SIZE = 128\n",
    "SAMPLING_RATE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from typing import Callable, Generator, List\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import contextlib\n",
    "\n",
    "\n",
    "def frame_windows(video_path: str,\n",
    "                  window_size: int,\n",
    "                  sampling_rate: int = 1\n",
    "                 ) -> Generator[List[np.ndarray], None, None]:\n",
    "    \"\"\"\n",
    "    Lazily read frames from the video, sampling 1 in every `sampling_rate`,\n",
    "    and yield lists of up to `window_size` sampled frames.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(f'Length of video: {cap.get(cv2.CAP_PROP_FRAME_COUNT)}')\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video file {video_path}\")\n",
    "        return None\n",
    "    frames: List[np.ndarray] = []\n",
    "    raw_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # only keep every `sampling_rate`th frame\n",
    "        if raw_idx % sampling_rate == 0:\n",
    "            frames.append(frame)\n",
    "            print()\n",
    "            if len(frames) == window_size:\n",
    "                yield frames\n",
    "                frames = []\n",
    "\n",
    "        raw_idx += 1\n",
    "\n",
    "    # leftover sampled frames\n",
    "    if frames:\n",
    "        yield frames\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "def extract_skeleton(video_path: str,\n",
    "                     window_size: int = 32,\n",
    "                     sampling_rate: int = 1,\n",
    "                     predict: Callable[[List[np.ndarray]], np.ndarray] = None\n",
    "                    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Process a video in non-overlapping windows to extract per-frame skeletons\n",
    "    without ever loading the whole video into RAM at once.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to the input video file.\n",
    "        window_size: Number of frames per chunk.\n",
    "        predict: Function(frames_list) → np.ndarray of shape\n",
    "                 (num_frames, num_keypoints, dims).\n",
    "\n",
    "    Returns:\n",
    "        skeleton_sequence: np.ndarray of shape\n",
    "            (total_frames, num_keypoints, dims).\n",
    "    \"\"\"\n",
    "    if predict is None:\n",
    "        raise ValueError(\"You must pass in a `predict` function.\")\n",
    "    \n",
    "    skeleton_chunks = []\n",
    "    total = 0\n",
    "    for frames in frame_windows(video_path, window_size, sampling_rate):\n",
    "        # frames is at most window_size in length\n",
    "\n",
    "# Suppose this is the noisy function:\n",
    "# from some_lib import noisy_function\n",
    "\n",
    "        with open(os.devnull, 'w') as devnull, \\\n",
    "            contextlib.redirect_stdout(devnull), \\\n",
    "            contextlib.redirect_stderr(devnull):\n",
    "            # Anything printed to stdout or stderr inside here is discarded\n",
    "            \n",
    "\n",
    "            det_results, _ = detection_inference(str(DETECTION_CONFIG), str(DETECTION_CHECKPOINT), frames)\n",
    "            pose_results, pose_data_samples = pose_inference(str(SKELETON_CONFIG), str(SKELETON_CHECKPOINT), frames, det_results)\n",
    "            # e.g. (len(frames), K, D)\n",
    "            skeleton_chunks.append(np.array(pose_results))\n",
    "            total += len(pose_results)\n",
    "        print(total)\n",
    "    \n",
    "    # Concatenate along temporal axis:\n",
    "    if len(skeleton_chunks) == 0:\n",
    "        skeleton_sequence = np.array([])\n",
    "    else:\n",
    "        skeleton_sequence = np.concatenate(skeleton_chunks, axis=0)\n",
    "    assert skeleton_sequence.shape[0] == total\n",
    "\n",
    "    return skeleton_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_skeleton = np.load(\"test_skeleton.npy\", allow_pickle=True)\n",
    "test_skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_skeletons(poses, skeleton_model, pose_inference):\n",
    "    \"\"\"\n",
    "    Process a batch of skeletons.\n",
    "    Args:\n",
    "        poses: list of dicts with keys \"ego\" and \"exo\"\n",
    "        pose_inference: function to extract skeletons\n",
    "    Returns:\n",
    "        batch: list of tensors with shape (B, T, D)\n",
    "    \"\"\"\n",
    "    skeletons_batch = []\n",
    "    for data in poses:\n",
    "        paths = [path for path in data[\"samples\"][\"exo\"]]\n",
    "        paths.append(data[\"samples\"][\"ego\"])\n",
    "        \n",
    "        skeletons = {}\n",
    "        \n",
    "        for i, path in enumerate(paths):\n",
    "            # Extract skeletons from the video\n",
    "            skeletons[str(i)] = extract_skeleton(path, window_size = 128, sampling_rate=64, predict=pose_inference)\n",
    "        skeletons_batch.append(skeletons)\n",
    "    batch = []\n",
    "    \n",
    "    for sample in skeletons_batch:\n",
    "        views = []\n",
    "        for view in sample.values():\n",
    "            representation = inference_skeleton(skeleton_model, view,(1920,1080), test_pipeline=None)  # (B, T, D)\n",
    "            views.append(representation)  # (B, T, D)\n",
    "        batch.append(torch.stack(views))  # (B, T, D)\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "logging.getLogger('mmdet').setLevel(logging.WARNING)\n",
    "# Now INFO and DEBUG from some_lib won’t show up.\n",
    "\n",
    "\n",
    "\n",
    "class MultiViewSkeletonClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    PyTorch model for multi-view skeleton fusion and classification.\n",
    "\n",
    "    Inputs:\n",
    "        - Four skeleton feature tensors, each of shape (batch_size, seq_len=20, feat_dim=512).\n",
    "    Output:\n",
    "        - Logits for classification into num_classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, skeleton_model, feat_dim=512, seq_len=20, num_views=4, hidden_dim=512, num_classes=10):\n",
    "        super(MultiViewSkeletonClassifier, self).__init__()\n",
    "        self.num_views = num_views\n",
    "        self.seq_len = seq_len\n",
    "        self.feat_dim = feat_dim\n",
    "        self.skeleton_model = skeleton_model\n",
    "        \n",
    "        # Define the attention head for per-view summary\n",
    "        self.attn_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # Learnable fusion weights for each view (softmax-normalized)\n",
    "        self.view_weights = nn.Parameter(torch.ones(num_views))\n",
    "\n",
    "        # Project concatenated features to hidden dimension\n",
    "        self.fusion_proj = nn.Linear(feat_dim, hidden_dim)\n",
    "\n",
    "        # Temporal convolutional layers\n",
    "        self.conv1 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * seq_len, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            views: list or tuple of length num_views, each tensor of shape (B, T, D)\n",
    "        Returns:\n",
    "            logits: tensor of shape (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Stack views: (V, B, T, D) -> (B, V, T, D)\n",
    "        \n",
    "            \n",
    "        x = torch.stack(batch).to(device='cuda:0')  # (B, V, T, D)\n",
    "        B, V, T, D = x.shape\n",
    "        \n",
    "        # per‐view summary → score → masked softmax → fusion\n",
    "        summary = x.mean(dim=2)              # (B, V, hidden)\n",
    "        raw_w   = self.attn_head(summary).squeeze(-1) # (B, V)\n",
    "        w       = F.softmax(raw_w)    # (B, V)\n",
    "        fused   = (w.view(B,V,1,1) * x).sum(dim=1)\n",
    "\n",
    "\n",
    "        # Project features\n",
    "        fused = self.fusion_proj(fused)  # (B, T, hidden_dim)\n",
    "        fused = F.relu(fused)\n",
    "\n",
    "        # Prepare for temporal conv: (B, hidden_dim, T)\n",
    "        fused = fused.permute(0, 2, 1)\n",
    "\n",
    "        # Temporal convolutional block\n",
    "        out = F.relu(self.bn1(self.conv1(fused)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "\n",
    "        # Flatten temporal features\n",
    "        out = out.view(B, -1)\n",
    "\n",
    "        # Classification head\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    B, T, D = 8, 20, 512\n",
    "    num_views = 4\n",
    "    num_classes = 4\n",
    "    multiview_model = MultiViewSkeletonClassifier(skeleton_model=skeleton_model, feat_dim=512, seq_len=T, num_views=num_views, hidden_dim=D, num_classes=num_classes).to('cuda:0')\n",
    "    # Create dummy inputs\n",
    "    views = [torch.randn(B, T, D) for _ in range(num_views)]\n",
    "    \n",
    "    batch = process_skeletons(([data, data]), skeleton_model, pose_inference)\n",
    "    results = multiview_model(batch)  # Forward pass\n",
    "    \n",
    "    print(\"Input shapes:\", [view.device for view in views])  # expected (8, 20, 512)\n",
    "    print(results)\n",
    "     # expected (8, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs    = 30\n",
    "learning_rate = 0.001\n",
    "batch_size    = 8\n",
    "num_classes   = 5\n",
    "\n",
    "# Model\n",
    "multiview_model = MultiViewSkeletonClassifier(\n",
    "    skeleton_model=skeleton_model,\n",
    "    feat_dim=512,\n",
    "    seq_len=20,\n",
    "    num_views=4,\n",
    "    hidden_dim=512,\n",
    "    num_classes=num_classes\n",
    ").to(device)\n",
    "model = multiview_model\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn=collate_objects,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,  # assuming you have a separate validation dataset\n",
    "    collate_fn=collate_objects,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Training + Validation loop\n",
    "for epoch in range(num_epochs):\n",
    "    # ---- TRAIN ----\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for views, labels in tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # if views is a list of tensors; otherwise .to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward + backward\n",
    "        outputs = model(views)\n",
    "        loss    = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # stats\n",
    "        running_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        total   += labels.size(0)\n",
    "        correct += preds.eq(labels).sum().item()\n",
    "        \n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc  = 100. * correct / total\n",
    "\n",
    "    # ---- VALIDATION ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for views, labels in tqdm(val_loader, desc=f\"Val   Epoch {epoch+1}/{num_epochs}\"):\n",
    "            \n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(views)\n",
    "            loss    = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, preds = outputs.max(1)\n",
    "            val_total   += labels.size(0)\n",
    "            val_correct += preds.eq(labels).sum().item()\n",
    "\n",
    "    val_loss_epoch = val_loss / len(val_loader)\n",
    "    val_acc        = 100. * val_correct / val_total\n",
    "    scheduler.step(val_loss_epoch)\n",
    "    # ---- EPOCH SUMMARY ----\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
    "        f\"Val   Loss: {val_loss_epoch:.4f}, Val   Acc: {val_acc:.2f}%\"\n",
    "    )\n",
    "    torch.save(model.state_dict(), f\"multiview_model_epoch_{epoch+1}.pth\")\n",
    "print(\"Training complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loop\n",
    "test_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    collate_fn=collate_objects,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the trained model\n",
    "model.load_state_dict(torch.load(\"./ckpt/multiview_model_epoch_30.pth\"))\n",
    "model.eval()\n",
    "\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    videos = []\n",
    "    ego_model_predictions = []\n",
    "    exo_model_predictions = []\n",
    "    for views, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "        it_exo_model_predictions = []\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        videos += [views[i]['samples']['ego'].split(\"/\")[-5] for i in range(len(views))]\n",
    "        batch = process_skeletons(views, skeleton_model, pose_inference)\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        exo_model_predictions += [[outputs[i][0]] * 4 for i in range(len(outputs))]\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        _, preds = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += preds.eq(labels).sum().item()\n",
    "\n",
    "test_loss_epoch = test_loss / len(test_loader)\n",
    "test_acc = 100. * test_correct / test_total\n",
    "\n",
    "# ---- TEST SUMMARY ----\n",
    "print(\n",
    "    f\"Test Loss: {test_loss_epoch:.4f}, Test Acc: {test_acc:.2f}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ego-challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
